{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, date, timedelta\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates_and_times(mode=1, d1=None, d2=None, a_of_data=1000, a_of_date=None):\n",
    "    '''Create date tuple between d1(beginning date) and d2(ending date).'''\n",
    "    \n",
    "    # Mode 1: input d1 and the a_of_data to be downloaded\n",
    "    # Mode 2: input d1 and the a_of_date considered\n",
    "    # Mode 3: input d1 and d2\n",
    "    # Mode 4: input a_of_data, automatically start from the last date detected in the csv file\n",
    "    \n",
    "    data = pd.read_csv('weather_data_backup.csv')\n",
    "    csv_date = data.date.tolist()\n",
    "    csv_time = data.time.tolist()\n",
    "    # date_today = datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d')\n",
    "    \n",
    "    def mode_2_3(d1, delta):\n",
    "        \n",
    "        dates_and_times = ()\n",
    "        \n",
    "        for i in range(delta):\n",
    "            time_tuple = (d1 + timedelta(days=i)).timetuple()\n",
    "            for j in range(24):\n",
    "                point_five_hour = (time_tuple[0], time_tuple[1], time_tuple[2], j, 30, 0, 0, 1)\n",
    "                dates_and_times += int(time.mktime(point_five_hour.timetuple),)\n",
    "                \n",
    "        return dates_and_times\n",
    "            \n",
    "    def mode_1_4(d1, a_of_data):\n",
    "        \n",
    "        i = 1\n",
    "        d_increment = 0\n",
    "        h_increment = 0\n",
    "        dates_and_times = ()\n",
    "        \n",
    "        while i <= a_of_data:\n",
    "            \n",
    "            time_tuple = (d1 + timedelta(days=d_increment)).timetuple()\n",
    "            time_stamp = int(time.mktime(time_tuple))\n",
    "            \n",
    "            current_date = str(datetime.fromtimestamp(time_stamp).strftime('%Y-%m-%d'))\n",
    "            current_time = '{}:30:00'.format(str(h_increment).zfill(2))\n",
    "\n",
    "            checker = True\n",
    "            for c_d, c_t in zip(csv_date, csv_time):\n",
    "\n",
    "                if current_date == c_d and current_time == c_t: \n",
    "                    \n",
    "                    checker = False\n",
    "                    \n",
    "            # if current_date == date_today: checker = False\n",
    "            \n",
    "            if checker:\n",
    "                \n",
    "                point_five_hour = (time_tuple[0], time_tuple[1], time_tuple[2], h_increment, 30, 0, 0, 0, 1)\n",
    "                dates_and_times += (int(time.mktime(point_five_hour)),)\n",
    "\n",
    "                i += 1\n",
    "            \n",
    "            h_increment += 1\n",
    "            \n",
    "            if h_increment == 24:\n",
    "                h_increment = 0\n",
    "                d_increment += 1\n",
    "                \n",
    "        return dates_and_times\n",
    "    \n",
    "    \n",
    "    if mode == 1: \n",
    "        dates_and_times = mode_1_4(d1, a_of_data)\n",
    "        \n",
    "    if mode == 2: \n",
    "        dates_and_times = mode_2_3(d1, a_of_date)\n",
    "    \n",
    "    if mode == 3: \n",
    "        delta = (d2 - d1).days + 1\n",
    "        dates_and_times = mode_2_3(d1, delta)\n",
    "        \n",
    "    if mode == 4: \n",
    "        temp = (data['date'][len(data)-1]).split('-')\n",
    "        d1 = date(int(temp[0]), int(temp[1]), int(temp[2]))\n",
    "        dates_and_times = mode_1_4(d1, a_of_data)\n",
    "            \n",
    "    print(str(len(dates_and_times)) + \" times of download will be executed.\")\n",
    "    print('-' * 100)\n",
    "        \n",
    "    return dates_and_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplication(date, time):\n",
    "    '''Check if there is the same item in the csv file 'weather_data_backup.csv'.'''\n",
    "    \n",
    "    csv_file_name = 'weather_data_backup.csv'\n",
    "    \n",
    "    data = pd.read_csv(csv_file_name)\n",
    "    csv_date = data.date.tolist()\n",
    "    csv_time = data.time.tolist()\n",
    "    \n",
    "    # Empty csv file will trigger the importation immediately\n",
    "    if not csv_date and not csv_time:\n",
    "        return True\n",
    "    \n",
    "    checker = True\n",
    "    \n",
    "    # Loop through the csv file, one row with the same date and the same time will return False\n",
    "    for c_d, c_t in zip(csv_date, csv_time):\n",
    "        if date == c_d and time == c_t:\n",
    "            checker = False\n",
    "    \n",
    "    if checker:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data_list):\n",
    "    '''\n",
    "    Add new data to the csv file 'weather_data_backup.csv' \n",
    "    and sorted data to 'weather_data.csv'.\n",
    "    '''\n",
    "    \n",
    "    df = pd.DataFrame(data=data_list)\n",
    "    \n",
    "    with open('weather_data_backup.csv','a') as f:\n",
    "        df.to_csv(f, header=False, index=False)\n",
    "        \n",
    "    # Sort data in the csv file\n",
    "    data = pd.read_csv('weather_data_backup.csv')\n",
    "    sorted_data = data.sort_values(['date', 'time'])\n",
    "    df = pd.DataFrame(data=sorted_data)\n",
    "    headers = df.head\n",
    "    \n",
    "    with open('weather_data.csv', 'w') as f:\n",
    "        df.to_csv(f, header=headers, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(desc, progress, speed):\n",
    "    '''Progress indicator'''\n",
    "    \n",
    "    import time, sys\n",
    "    \n",
    "    length = 50\n",
    "    block = int(round(length*progress))\n",
    "    progress_str = '%0.1f'%(progress*100)\n",
    "    \n",
    "    progress_len = 5\n",
    "    progress_str = ' ' * (progress_len - len(progress_str)) + progress_str\n",
    "    \n",
    "    if block <= 1:\n",
    "        msg = \"\\r{0}: [{1}] {2}% | Spd: {3}s/date |\".format(desc, \">\" * block + \".\" * (length-block), \n",
    "                                                     progress_str, speed)\n",
    "    if 1 < block < 50:\n",
    "        msg = \"\\r{0}: [{1}] {2}% | Spd: {3}s/date |\".format(desc, \"=\" * (block - 1) + '>' + \".\" * (length-block), \n",
    "                                                     progress_str, speed)\n",
    "    \n",
    "    \n",
    "    if progress >= 1: msg = \"\\r{0}: [{1}] {2}% | Spd: {3}s/date | DONE\\r\\n\".format(desc, \"=\" * (block) + \".\" * (length-block), \n",
    "                                                                              progress_str, speed)\n",
    "\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_json_to_csv(converted_dates_and_times):\n",
    "    '''Download the json file for a specific hour and store it in a local csv file.'''\n",
    "    \n",
    "    api_key = # Enter your Darksky api key\n",
    "    longitude_and_latitude = '48.864716,2.349014'\n",
    "    api = 'https://api.darksky.net/forecast/' + api_key + '/' + longitude_and_latitude + ',{}' + str(time_stamp) + '?units=si'\n",
    "    \n",
    "    outer_time_start = time.time()\n",
    "    \n",
    "    date_today = datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Data headers wanted\n",
    "    data_headers = ['temperature', \n",
    "                    'apparentTemperature', \n",
    "                    'precipIntensity', \n",
    "                    'precipProbability',\n",
    "                    'dewPoint', \n",
    "                    'humidity', \n",
    "                    'pressure', \n",
    "                    'windSpeed', \n",
    "                    'cloudCover']\n",
    "    \n",
    "    previous_date = ''\n",
    "    Y_m_d = ()\n",
    "    H_M_S = ()\n",
    "    error_messages = ()\n",
    "    \n",
    "    for i in converted_dates_and_times:\n",
    "        Y_m_d += (str(datetime.fromtimestamp(i).strftime('%Y-%m-%d')),)\n",
    "    \n",
    "    # Main downloading and storing loop\n",
    "    for time_stamp in converted_dates_and_times:\n",
    "        \n",
    "        inner_start_time = time.time()\n",
    "        \n",
    "        current_date = str(datetime.fromtimestamp(time_stamp).strftime('%Y-%m-%d'))\n",
    "        current_time = str(datetime.fromtimestamp(time_stamp).strftime('%H:%M:%S'))\n",
    "        current_hour = str(datetime.fromtimestamp(time_stamp).strftime('%H'))\n",
    "        \n",
    "        if current_date == date_today:\n",
    "            print('Today is covered in the importation range, so the application is automatically stopped.')\n",
    "            break\n",
    "        \n",
    "        if current_date != previous_date:\n",
    "            iteration_count = 1\n",
    "            n_iteration = Y_m_d.count(current_date)\n",
    "            previous_date = current_date     \n",
    "        else:\n",
    "            iteration_count += 1\n",
    "\n",
    "        if check_duplication(current_date, current_time):\n",
    "            \n",
    "            # Get json data from Darksky's API\n",
    "            api = api.format(str(time_stamp))\n",
    "            # Handle the exception returned from the API\n",
    "            try:\n",
    "                json_data = json.loads(urlopen(api).read().decode())\n",
    "            except:\n",
    "                print('\\n')\n",
    "                print('Maximum calls per day is attained!')\n",
    "                break\n",
    "                \n",
    "            filtered_data = []\n",
    "        \n",
    "            # Loading the whole forecast for a day at 0 o'clock of that day\n",
    "            if current_hour == '00':\n",
    "                \n",
    "                for hourly_data in json_data['hourly']['data']:\n",
    "                    temp = []\n",
    "                    temp += ('forecast',)\n",
    "                    temp += (datetime.fromtimestamp(hourly_data['time']).strftime('%Y-%m-%d'),)\n",
    "                    temp += (datetime.fromtimestamp(hourly_data['time']).strftime('%H:%M:%S'),)\n",
    "                    \n",
    "                    for target in data_headers:\n",
    "                        temp += (hourly_data.get(target),)\n",
    "                    filtered_data.append(temp)\n",
    "                    \n",
    "            temp = []\n",
    "            temp += ('now',)\n",
    "            temp += (current_date,)\n",
    "            temp += (current_time,)\n",
    "            \n",
    "            for target in data_headers:\n",
    "                temp += ((json_data['currently']).get(target),)\n",
    "            filtered_data.append(temp)\n",
    "            save_to_csv(filtered_data)\n",
    "            inner_elapsed_time = time.time() - inner_start_time\n",
    "            \n",
    "        else:\n",
    "            error_messages += (current_date + ' ' + current_time + ' data already exists!',)\n",
    "            inner_elapsed_time = time.time() - inner_start_time\n",
    "            \n",
    "        if iteration_count == 1:\n",
    "            cumulated_time = inner_elapsed_time\n",
    "            importation_speed = round(cumulated_time, 2)\n",
    "        else:\n",
    "            cumulated_time += inner_elapsed_time\n",
    "            importation_speed = round(cumulated_time / iteration_count, 2)\n",
    "            \n",
    "        update_progress(current_date, iteration_count/n_iteration, '%0.2f'%(importation_speed))\n",
    "    \n",
    "    print('-' * 100)\n",
    "    \n",
    "    for i in error_messages:\n",
    "        print(i)\n",
    "        \n",
    "    if error_messages == ():\n",
    "        print('No error is recognized.')\n",
    "        \n",
    "    # Display summary of the importation\n",
    "    print('-' * 100)\n",
    "    print('Summary')\n",
    "    \n",
    "    outer_elapsed_time = time.time() - outer_time_start\n",
    "    m, s = divmod(outer_elapsed_time, 60)\n",
    "    \n",
    "    print('Total time consumed: {} minutes {} seconds'.format(round(m), round(s)))\n",
    "    data = pd.read_csv('weather_data.csv')\n",
    "    print('Total amount of data in the csv file: {}'.format(len(data)))\n",
    "    print('Current range of data: from {} to {}'.format(data['date'][0], data['date'][len(data) - 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 times of download will be executed.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2014-04-06: [==================================================] 100.0% | Spd: 2.93s/date | DONE\n",
      "2014-04-07: [==================================================] 100.0% | Spd: 2.58s/date | DONE\n",
      "2014-04-08: [==================================================] 100.0% | Spd: 2.65s/date | DONE\n",
      "2014-04-09: [==================================================] 100.0% | Spd: 2.01s/date | DONE\n",
      "2014-04-10: [==================================================] 100.0% | Spd: 1.69s/date | DONE\n",
      "2014-04-11: [==================================================] 100.0% | Spd: 1.61s/date | DONE\n",
      "2014-04-12: [==================================================] 100.0% | Spd: 1.57s/date | DONE\n",
      "2014-04-13: [==================================================] 100.0% | Spd: 1.53s/date | DONE\n",
      "2014-04-14: [==================================================] 100.0% | Spd: 1.55s/date | DONE\n",
      "2014-04-15: [==================================================] 100.0% | Spd: 1.53s/date | DONE\n",
      "2014-04-16: [==================================================] 100.0% | Spd: 1.58s/date | DONE\n",
      "2014-04-17: [==================================================] 100.0% | Spd: 1.54s/date | DONE\n",
      "2014-04-18: [==================================================] 100.0% | Spd: 1.47s/date | DONE\n",
      "2014-04-19: [==================================================] 100.0% | Spd: 1.52s/date | DONE\n",
      "2014-04-20: [==================================================] 100.0% | Spd: 2.05s/date | DONE\n",
      "2014-04-21: [==================================================] 100.0% | Spd: 1.93s/date | DONE\n",
      "2014-04-22: [==================================================] 100.0% | Spd: 1.91s/date | DONE\n",
      "2014-04-23: [==================================================] 100.0% | Spd: 1.92s/date | DONE\n",
      "2014-04-24: [==================================================] 100.0% | Spd: 1.94s/date | DONE\n",
      "2014-04-25: [==================================================] 100.0% | Spd: 2.14s/date | DONE\n",
      "2014-04-26: [==================================================] 100.0% | Spd: 2.25s/date | DONE\n",
      "2014-04-27: [==================================================] 100.0% | Spd: 1.80s/date | DONE\n",
      "2014-04-28: [==================================================] 100.0% | Spd: 1.81s/date | DONE\n",
      "2014-04-29: [==================================================] 100.0% | Spd: 2.04s/date | DONE\n",
      "2014-04-30: [==================================================] 100.0% | Spd: 2.88s/date | DONE\n",
      "2014-05-01: [==================================================] 100.0% | Spd: 2.72s/date | DONE\n",
      "2014-05-02: [==================================================] 100.0% | Spd: 2.56s/date | DONE\n",
      "2014-05-03: [==================================================] 100.0% | Spd: 2.44s/date | DONE\n",
      "2014-05-04: [==================================================] 100.0% | Spd: 2.54s/date | DONE\n",
      "2014-05-05: [==================================================] 100.0% | Spd: 2.58s/date | DONE\n",
      "2014-05-06: [==================================================] 100.0% | Spd: 2.72s/date | DONE\n",
      "2014-05-07: [==================================================] 100.0% | Spd: 2.61s/date | DONE\n",
      "2014-05-08: [==================================================] 100.0% | Spd: 2.31s/date | DONE\n",
      "2014-05-09: [==================================================] 100.0% | Spd: 2.48s/date | DONE\n",
      "2014-05-10: [==================================================] 100.0% | Spd: 2.61s/date | DONE\n",
      "2014-05-11: [==================================================] 100.0% | Spd: 2.51s/date | DONE\n",
      "2014-05-12: [==================================================] 100.0% | Spd: 2.83s/date | DONE\n",
      "2014-05-13: [==================================================] 100.0% | Spd: 2.93s/date | DONE\n",
      "2014-05-14: [==================================================] 100.0% | Spd: 2.38s/date | DONE\n",
      "2014-05-15: [==================================================] 100.0% | Spd: 1.64s/date | DONE\n",
      "2014-05-16: [==================================================] 100.0% | Spd: 1.58s/date | DONE\n",
      "2014-05-17: [==================================================] 100.0% | Spd: 1.57s/date | DONE\n",
      "2014-05-18: [==================================================] 100.0% | Spd: 1.54s/date | DONE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "No error is recognized.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Summary\n",
      "Total time consumed: 35 minutes 19 seconds\n",
      "Total amount of data in the csv file: 65781\n",
      "Current range of data: from 2013-12-31 to 2018-05-17\n"
     ]
    }
   ],
   "source": [
    "converted_dates_and_times = get_dates_and_times(mode=1, d1=date(2014, 4, 5), a_of_data=1000)\n",
    "from_json_to_csv(converted_dates_and_times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
