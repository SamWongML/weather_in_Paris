{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, date, timedelta\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates_and_times(mode=1, d1=None, d2=None, a_of_data=1000, a_of_date=None):\n",
    "    '''Create date tuple between d1(beginning date) and d2(ending date).\n",
    "    \n",
    "    Mode 1: input d1 and the a_of_data to be downloaded\n",
    "    Mode 2: input d1 and the a_of_date considered\n",
    "    Mode 3: input d1 and d2\n",
    "    Mode 4: input a_of_data, automatically start from the last date detected in the csv file\n",
    "    '''\n",
    "    \n",
    "    data = pd.read_csv('weather_data_backup.csv')\n",
    "    csv_date = data.date.tolist()\n",
    "    csv_time = data.time.tolist()\n",
    "    # date_today = datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d')\n",
    "    \n",
    "    def mode_2_3(d1, delta):\n",
    "        \n",
    "        dates_and_times = ()\n",
    "        \n",
    "        for i in range(delta):\n",
    "            time_tuple = (d1 + timedelta(days=i)).timetuple()\n",
    "            for j in range(24):\n",
    "                point_five_hour = (time_tuple[0], time_tuple[1], time_tuple[2], j, 30, 0, 0, 1)\n",
    "                dates_and_times += int(time.mktime(point_five_hour.timetuple),)\n",
    "                \n",
    "        return dates_and_times\n",
    "            \n",
    "    def mode_1_4(d1, a_of_data):\n",
    "        \n",
    "        i = 1\n",
    "        d_increment = 0\n",
    "        h_increment = 0\n",
    "        dates_and_times = ()\n",
    "        \n",
    "        while i <= a_of_data:\n",
    "            \n",
    "            time_tuple = (d1 + timedelta(days=d_increment)).timetuple()\n",
    "            time_stamp = int(time.mktime(time_tuple))\n",
    "            \n",
    "            current_date = str(datetime.fromtimestamp(time_stamp).strftime('%Y-%m-%d'))\n",
    "            current_time = '{}:30:00'.format(str(h_increment).zfill(2))\n",
    "\n",
    "            checker = True\n",
    "            for c_d, c_t in zip(csv_date, csv_time):\n",
    "\n",
    "                if current_date == c_d and current_time == c_t: \n",
    "                    \n",
    "                    checker = False\n",
    "                    \n",
    "            # if current_date == date_today: checker = False\n",
    "            \n",
    "            if checker:\n",
    "                \n",
    "                point_five_hour = (time_tuple[0], time_tuple[1], time_tuple[2], h_increment, 30, 0, 0, 0, 1)\n",
    "                dates_and_times += (int(time.mktime(point_five_hour)),)\n",
    "\n",
    "                i += 1\n",
    "            \n",
    "            h_increment += 1\n",
    "            \n",
    "            if h_increment == 24:\n",
    "                h_increment = 0\n",
    "                d_increment += 1\n",
    "                \n",
    "        return dates_and_times\n",
    "    \n",
    "    \n",
    "    if mode == 1: \n",
    "        dates_and_times = mode_1_4(d1, a_of_data)\n",
    "        \n",
    "    if mode == 2: \n",
    "        dates_and_times = mode_2_3(d1, a_of_date)\n",
    "    \n",
    "    if mode == 3: \n",
    "        delta = (d2 - d1).days + 1\n",
    "        dates_and_times = mode_2_3(d1, delta)\n",
    "        \n",
    "    if mode == 4: \n",
    "        temp = (data['date'][len(data)-1]).split('-')\n",
    "        d1 = date(int(temp[0]), int(temp[1]), int(temp[2]))\n",
    "        dates_and_times = mode_1_4(d1, a_of_data)\n",
    "            \n",
    "    print(str(len(dates_and_times)) + \" times of download will be executed.\")\n",
    "    print('-' * 100)\n",
    "        \n",
    "    return dates_and_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplication(date, time):\n",
    "    '''Check if there is the same item in the csv file 'weather_data_backup.csv'.'''\n",
    "    \n",
    "    csv_file_name = 'weather_data_backup.csv'\n",
    "    \n",
    "    data = pd.read_csv(csv_file_name)\n",
    "    csv_date = data.date.tolist()\n",
    "    csv_time = data.time.tolist()\n",
    "    \n",
    "    # Empty csv file will trigger the importation immediately\n",
    "    if not csv_date and not csv_time:\n",
    "        return True\n",
    "    \n",
    "    checker = True\n",
    "    \n",
    "    # Loop through the csv file, one row with the same date and the same time will return False\n",
    "    for c_d, c_t in zip(csv_date, csv_time):\n",
    "        if date == c_d and time == c_t:\n",
    "            checker = False\n",
    "    \n",
    "    if checker:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data_list):\n",
    "    '''\n",
    "    Add new data to the csv file 'weather_data_backup.csv' \n",
    "    and sorted data to 'weather_data.csv'.\n",
    "    '''\n",
    "    \n",
    "    df = pd.DataFrame(data=data_list)\n",
    "    \n",
    "    with open('weather_data_backup.csv','a') as f:\n",
    "        df.to_csv(f, header=False, index=False)\n",
    "        \n",
    "    # Sort data in the csv file\n",
    "    data = pd.read_csv('weather_data_backup.csv')\n",
    "    sorted_data = data.sort_values(['date', 'time'])\n",
    "    df = pd.DataFrame(data=sorted_data)\n",
    "    headers = df.head\n",
    "    \n",
    "    with open('weather_data.csv', 'w') as f:\n",
    "        df.to_csv(f, header=headers, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(desc, progress, speed):\n",
    "    '''Progress indicator'''\n",
    "    \n",
    "    import time, sys\n",
    "    \n",
    "    length = 50\n",
    "    block = int(round(length*progress))\n",
    "    progress_str = '%0.1f'%(progress*100)\n",
    "    \n",
    "    progress_len = 5\n",
    "    progress_str = ' ' * (progress_len - len(progress_str)) + progress_str\n",
    "    \n",
    "    if block <= 1:\n",
    "        msg = \"\\r{0}: [{1}] {2}% | Spd: {3}s/date |\".format(desc, \">\" * block + \".\" * (length-block), \n",
    "                                                     progress_str, speed)\n",
    "    if 1 < block < 50:\n",
    "        msg = \"\\r{0}: [{1}] {2}% | Spd: {3}s/date |\".format(desc, \"=\" * (block - 1) + '>' + \".\" * (length-block), \n",
    "                                                     progress_str, speed)\n",
    "    \n",
    "    \n",
    "    if progress >= 1: msg = \"\\r{0}: [{1}] {2}% | Spd: {3}s/date | DONE\\r\\n\".format(desc, \"=\" * (block) + \".\" * (length-block), \n",
    "                                                                              progress_str, speed)\n",
    "\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_json_to_csv(converted_dates_and_times):\n",
    "    '''Download the json file for a specific hour and store it in a local csv file.'''\n",
    "    \n",
    "    api_key = '3fa6b62f319752474af8fc227335a5cc'\n",
    "    longitude_and_latitude = '48.864716,2.349014'\n",
    "    raw_api = 'https://api.darksky.net/forecast/' + api_key + '/' + longitude_and_latitude + ',{}' + '?units=si'\n",
    "    \n",
    "    outer_time_start = time.time()\n",
    "    \n",
    "    date_today = datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Data headers wanted\n",
    "    data_headers = ['temperature', \n",
    "                    'apparentTemperature', \n",
    "                    'precipIntensity', \n",
    "                    'precipProbability',\n",
    "                    'dewPoint', \n",
    "                    'humidity', \n",
    "                    'pressure', \n",
    "                    'windSpeed', \n",
    "                    'cloudCover']\n",
    "    \n",
    "    previous_date = ''\n",
    "    Y_m_d = ()\n",
    "    H_M_S = ()\n",
    "    error_messages = ()\n",
    "    \n",
    "    for i in converted_dates_and_times:\n",
    "        Y_m_d += (str(datetime.fromtimestamp(i).strftime('%Y-%m-%d')),)\n",
    "    \n",
    "    # Main downloading and storing loop\n",
    "    for time_stamp in converted_dates_and_times:\n",
    "        \n",
    "        inner_start_time = time.time()\n",
    "        \n",
    "        current_date = str(datetime.fromtimestamp(time_stamp).strftime('%Y-%m-%d'))\n",
    "        current_time = str(datetime.fromtimestamp(time_stamp).strftime('%H:%M:%S'))\n",
    "        current_hour = str(datetime.fromtimestamp(time_stamp).strftime('%H'))\n",
    "        \n",
    "        if current_date == date_today:\n",
    "            print('Today is covered in the importation range, so the application is automatically stopped.')\n",
    "            break\n",
    "        \n",
    "        if current_date != previous_date:\n",
    "            iteration_count = 1\n",
    "            n_iteration = Y_m_d.count(current_date)\n",
    "            previous_date = current_date     \n",
    "        else:\n",
    "            iteration_count += 1\n",
    "\n",
    "        if check_duplication(current_date, current_time):\n",
    "            \n",
    "            # Get json data from Darksky's API\n",
    "            api = raw_api.format(str(time_stamp))\n",
    "            # Handle the exception returned from the API\n",
    "            try:\n",
    "                json_data = json.loads(urlopen(api).read().decode())\n",
    "            except:\n",
    "                print('\\n')\n",
    "                print('Maximum calls per day is attained!')\n",
    "                break\n",
    "                \n",
    "            filtered_data = []\n",
    "        \n",
    "            # Loading the whole forecast for a day at 0 o'clock of that day\n",
    "            if current_hour == '00':\n",
    "                \n",
    "                for hourly_data in json_data['hourly']['data']:\n",
    "                    temp = []\n",
    "                    temp += ('forecast',)\n",
    "                    temp += (datetime.fromtimestamp(hourly_data['time']).strftime('%Y-%m-%d'),)\n",
    "                    temp += (datetime.fromtimestamp(hourly_data['time']).strftime('%H:%M:%S'),)\n",
    "                    \n",
    "                    for target in data_headers:\n",
    "                        temp += (hourly_data.get(target),)\n",
    "                    filtered_data.append(temp)\n",
    "                    \n",
    "            temp = []\n",
    "            temp += ('now',)\n",
    "            temp += (current_date,)\n",
    "            temp += (current_time,)\n",
    "            \n",
    "            for target in data_headers:\n",
    "                temp += ((json_data['currently']).get(target),)\n",
    "            filtered_data.append(temp)\n",
    "            save_to_csv(filtered_data)\n",
    "            inner_elapsed_time = time.time() - inner_start_time\n",
    "            \n",
    "        else:\n",
    "            error_messages += (current_date + ' ' + current_time + ' data already exists!',)\n",
    "            inner_elapsed_time = time.time() - inner_start_time\n",
    "            \n",
    "        if iteration_count == 1:\n",
    "            cumulated_time = inner_elapsed_time\n",
    "            importation_speed = round(cumulated_time, 2)\n",
    "        else:\n",
    "            cumulated_time += inner_elapsed_time\n",
    "            importation_speed = round(cumulated_time / iteration_count, 2)\n",
    "            \n",
    "        update_progress(current_date, iteration_count/n_iteration, '%0.2f'%(importation_speed))\n",
    "    \n",
    "    print('-' * 100)\n",
    "    \n",
    "    for i in error_messages:\n",
    "        print(i)\n",
    "        \n",
    "    if error_messages == ():\n",
    "        print('No error is recognized.')\n",
    "        \n",
    "    # Display summary of the importation\n",
    "    print('-' * 100)\n",
    "    print('Summary')\n",
    "    \n",
    "    outer_elapsed_time = time.time() - outer_time_start\n",
    "    m, s = divmod(outer_elapsed_time, 60)\n",
    "    \n",
    "    print('Total time consumed: {} minutes {} seconds'.format(round(m), round(s)))\n",
    "    data = pd.read_csv('weather_data.csv')\n",
    "    print('Total amount of data in the csv file: {}'.format(len(data)))\n",
    "    print('Current range of data: from {} to {}'.format(data['date'][0], data['date'][len(data) - 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 times of download will be executed.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2014-06-27: [==================================================] 100.0% | Spd: 1.49s/date | DONE\n",
      "2014-06-28: [==================================================] 100.0% | Spd: 1.47s/date | DONE\n",
      "2014-06-29: [==================================================] 100.0% | Spd: 1.83s/date | DONE\n",
      "2014-06-30: [==================================================] 100.0% | Spd: 1.52s/date | DONE\n",
      "2014-07-01: [==================================================] 100.0% | Spd: 1.50s/date | DONE\n",
      "2014-07-02: [==================================================] 100.0% | Spd: 1.47s/date | DONE\n",
      "2014-07-03: [==================================================] 100.0% | Spd: 1.51s/date | DONE\n",
      "2014-07-04: [==================================================] 100.0% | Spd: 1.53s/date | DONE\n",
      "2014-07-05: [==================================================] 100.0% | Spd: 1.65s/date | DONE\n",
      "2014-07-06: [==================================================] 100.0% | Spd: 1.49s/date | DONE\n",
      "2014-07-07: [==================================================] 100.0% | Spd: 1.49s/date | DONE\n",
      "2014-07-08: [==================================================] 100.0% | Spd: 1.48s/date | DONE\n",
      "2014-07-09: [==================================================] 100.0% | Spd: 1.47s/date | DONE\n",
      "2014-07-10: [==================================================] 100.0% | Spd: 1.48s/date | DONE\n",
      "2014-07-11: [==================================================] 100.0% | Spd: 1.47s/date | DONE\n",
      "2014-07-12: [==================================================] 100.0% | Spd: 1.48s/date | DONE\n",
      "2014-07-13: [==================================================] 100.0% | Spd: 1.55s/date | DONE\n",
      "2014-07-14: [==================================================] 100.0% | Spd: 1.48s/date | DONE\n",
      "2014-07-15: [==================================================] 100.0% | Spd: 1.47s/date | DONE\n",
      "2014-07-16: [==================================================] 100.0% | Spd: 1.78s/date | DONE\n",
      "2014-07-17: [==================================================] 100.0% | Spd: 1.89s/date | DONE\n",
      "2014-07-18: [==================================================] 100.0% | Spd: 1.89s/date | DONE\n",
      "2014-07-19: [==================================================] 100.0% | Spd: 1.99s/date | DONE\n",
      "2014-07-20: [==================================================] 100.0% | Spd: 1.93s/date | DONE\n",
      "2014-07-21: [==================================================] 100.0% | Spd: 1.99s/date | DONE\n",
      "2014-07-22: [==================================================] 100.0% | Spd: 1.92s/date | DONE\n",
      "2014-07-23: [==================================================] 100.0% | Spd: 1.88s/date | DONE\n",
      "2014-07-24: [==================================================] 100.0% | Spd: 2.04s/date | DONE\n",
      "2014-07-25: [==================================================] 100.0% | Spd: 1.83s/date | DONE\n",
      "2014-07-26: [==================================================] 100.0% | Spd: 1.74s/date | DONE\n",
      "2014-07-27: [==================================================] 100.0% | Spd: 2.06s/date | DONE\n",
      "2014-07-28: [==================================================] 100.0% | Spd: 1.94s/date | DONE\n",
      "2014-07-29: [==================================================] 100.0% | Spd: 1.73s/date | DONE\n",
      "2014-07-30: [==================================================] 100.0% | Spd: 1.76s/date | DONE\n",
      "2014-07-31: [==================================================] 100.0% | Spd: 1.83s/date | DONE\n",
      "2014-08-01: [==================================================] 100.0% | Spd: 1.73s/date | DONE\n",
      "2014-08-02: [==================================================] 100.0% | Spd: 1.83s/date | DONE\n",
      "2014-08-03: [==================================================] 100.0% | Spd: 1.75s/date | DONE\n",
      "2014-08-04: [==================================================] 100.0% | Spd: 1.81s/date | DONE\n",
      "2014-08-05: [==================================================] 100.0% | Spd: 1.66s/date | DONE\n",
      "2014-08-06: [==================================================] 100.0% | Spd: 1.62s/date | DONE\n",
      "2014-08-07: [==================================================] 100.0% | Spd: 1.58s/date | DONE\n",
      "2014-08-08: [==================================================] 100.0% | Spd: 1.52s/date | DONE\n",
      "----------------------------------------------------------------------------------------------------\n",
      "No error is recognized.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Summary\n",
      "Total time consumed: 28 minutes 14 seconds\n",
      "Total amount of data in the csv file: 68280\n",
      "Current range of data: from 2013-12-31 to 2018-05-17\n"
     ]
    }
   ],
   "source": [
    "converted_dates_and_times = get_dates_and_times(mode=1, d1=date(2014,8, 8), a_of_data=1000)\n",
    "from_json_to_csv(converted_dates_and_times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
